{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRt0XWYN379z"
   },
   "source": [
    "# TensorFlow Datasets\n",
    "In this assignment you will create a data pipeline with for a cool Rock Paper Scissors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oJ56MwEm4Ye0"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (C:\\Users\\shomalco\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_tfds_logging\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_metadata \u001b[38;5;28;01mas\u001b[39;00m _call_metadata\n\u001b[0;32m     46\u001b[0m _metadata \u001b[38;5;241m=\u001b[39m _call_metadata\u001b[38;5;241m.\u001b[39mCallMetadata()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (C:\\Users\\shomalco\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2lwHbSY4O29"
   },
   "source": [
    "## 1. Loading Dataset\n",
    "load the Rock Paper Scissors Dataset from TFDS:\n",
    "https://www.tensorflow.org/datasets/catalog/rock_paper_scissors \\\n",
    "This Dataset have two splits: Train, Test\\\n",
    "In here we want three splits: Train, Validation and Test. So we should use TFDS split method to keep 80% of original Train split for training set, remaining 20% for validation set and all of the original Test split for test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_twAg52B4ObY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (train_ds, val_ds, test_ds), metadata \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_flowers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:90\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[90\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:]\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m     with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p7Np9995wzy"
   },
   "source": [
    "## 2. Inspecting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1hgDFJX3Ove"
   },
   "outputs": [],
   "source": [
    "num_classes = metadata.features['label'].num_classes\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "print(f\"Number of classes in the dataset: {num_classes}\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"class id {i} is {get_label_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClgZN9H36nsB"
   },
   "source": [
    "As you see we have three classes for this dataset.\n",
    "Now use `tfds.visualization.show_example` function to viusalize some data from training dataset.\\\n",
    "https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRkkvA0V6naX"
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_nRTVFN7rfv"
   },
   "source": [
    "## 3. Preprocessing\n",
    "Now write a python function that resize images to `224x224` and rescale the pixels to be between  `[0,1]` (You can use [tf.image](https://www.tensorflow.org/api_docs/python/tf/image/resize) and [tf.divide](https://https://www.tensorflow.org/api_docs/python/tf/math/divide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-kcF-If5906"
   },
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    #Code Here\n",
    "    return image, encoded_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBzFY0qw9QnR"
   },
   "source": [
    "Now by using `map` method apply this preprocessing function to all splits: (use `num_parallel_calls` argument in map function to run the function faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHrewtfb9OdF"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = #Code Here\n",
    "val_ds = #Code Here\n",
    "test_ds = #Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "az_lE5rU-pNu"
   },
   "source": [
    "## 4. Data Augmentation\n",
    "Create a sequential model with three augmentation layer:\n",
    "1. [Random Flip](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/)\n",
    "2. [Random Rotation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/)\n",
    "3. [Random Zoom](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSd78hnQ9-CO"
   },
   "outputs": [],
   "source": [
    "data_augmentation = #Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LPlIiSiABLV"
   },
   "source": [
    "Use `map` method to apply these augmentation layers to the training data set. (use `num_parallel_calls` argument in map function to run the function faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjglJTfz__Sp"
   },
   "outputs": [],
   "source": [
    "train_ds = #Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ISBB3HJBizI"
   },
   "source": [
    "## 5. Shuffle and Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQIadQrFBmqp"
   },
   "source": [
    "Shuffle training data and batch all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9k7jY0u0AfsL"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "buffer_size = 1000\n",
    "train_ds = #Code Here\n",
    "train_ds = #Code Here\n",
    "val_ds = #Code Here\n",
    "test_ds = #Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Gt5ODvgKQtg"
   },
   "source": [
    "Add cache and prefetch for optimizing the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xB02Wb9KMxk"
   },
   "outputs": [],
   "source": [
    "train_ds = #Code Here\n",
    "val_ds = #Code Here\n",
    "test_ds = #Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTOWw4sGLEF2"
   },
   "source": [
    "## 6. Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbTOGrNJKilK"
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "num_classes = 3\n",
    "\n",
    "input = tf.keras.Input(shape=input_shape)\n",
    "x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", name=\"first_conv\")(input)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "pred = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QyxwQcBLRvA"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "\n",
    "test_scores = model.evaluate(test_ds, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6fcJaVQLWnU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMreyIy8Rm36xmN2hHIX4Nh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
